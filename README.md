# Enhancing-OTP-Security-using-ML
Creating a ML powered System that can prevent SIM-swap attacks on OTP Systems

#Introduction
One-time Password-based authentication stands out to be the most effective in the cluster of password-less authentication systems. Why can we use it as an authentication factor for login rather than an account recovery mechanism? Recent studies show that attacks like sim-swap and device theft raise a significant threat for the system. In this paper, a new security system is proposed to prevent attacks like sim-swap on OTP systems, the system contains a risk engine made up of supervised and unsupervised machine learning model blocks trained using genuine user data space, and the final decision of the system is subject to a decision block that works on the principles of voting and logic of an AND Gate. The proposed system performed well in detecting fraud users, proving the system's significance in solving the problems faced by an OTP system.

#How it works!
In a traditional OTP system, the user is granted access if both username and OTP are valid, and it is often used as an account recovery method. We can use OTP for authentication in the first place rather than as a recovery measure. It is found to be satisfactory for the users when authentication systems are replaced with the 'forgot password' recovery method.The proposed system uses OTP authentication in the login phase. Its mechanisms can be grouped into three main components, the Data extraction, Risk Engine, and Decision Block
![image](https://user-images.githubusercontent.com/71520074/146628679-a49c9afc-465b-4c0a-9184-a25e8687d117.png)
The Supervised learning blocks contain Support Vector Machine, K-Nearest Neighbors, and Naïve Bayes supervised machine learning algorithms trained using historical data containing user parameters, keystroke data, and class label column.SVM classifier is one of the best-supervised learning classifiers. It can solve both linear and nonlinear problems and is helpful for a wide range of applications. The working principle of SVM is simple: The algorithm creates a line or a hyper-plane which separates the data into classes. Here the model studies the underlying patterns in the data space and creates a hyperplane that divides genuine user data points from fraudulent user data points such that when an unseen data point comes, the model can correctly choose the side of the hyper-plane that a point belongs to in our case SVM classifies the new data point as either genuine or fraudulent user. KNN model is a distance-dependent model depending on the neighbours we are assigning. It studies underlying patterns from training data and assumes similarity between new cases and existing cases, and categorizes new cases into a class that is most similar in the available classes. Unlike SVM and KNN, Naïve Bayes uses Bayes theorem to decide the class that a parameter set belongs to. It calculates the probability of genuine class given parameter set with the probability of fraudulent class given parameter set and labels the parameter with a class with greater posterior probability. That is, P(Genuine/Parameter set) > P(Fraudulent/Parameter set) implies the given parameter set will be labelled as a genuine case. Thus the supervised learning block produces three outputs from the three models used, and the output falls into one of the entries in the output class ( genuine, fraud). Initially there won't be any data of genuine user for training the models, therefore the proposed model is under the assumption that records of genuine user already exists and used the same to train the models before it starts to classify users.
The unsupervised Learning block is powered by three unsupervised learning models: One-class Support Vector Machine, Isolation Forest, and Minimum Covariance Determinant. The primary objective of this model is to identify the outlier in the given data that is fraud entries. To make the model call fraud cases as outliers, the model should be trained on a dataset containing only genuine cases, as shown in fig 2. The output of an outlier detection model is 1 or -1, representing an inlier or outlier, respectively. The system considers an inlier as a genuine user and an outlier as a fraudulent user. Thus the Unsupervised learning block produces three outputs from the output class ( genuine, fraud). The F1-Score is the harmonic mean of Recall and Precision, and it provides a more accurate picture of cases that were mistakenly classified than the Accuracy Metric. Even though most models do not give an ideal F1-Score, the system performs well for the task at hand. The final system prediction is dependent on predictions made by models of both supervised and unsupervised learning blocks. Table 1 shows model performance based on F1-Score.
Model Name	Precision	Recall	F1-Score 
SVM	95%	92%	93%
Naïve Bayes	86%	86%	86%
KNN	94%	92%	93%
One-Class SVM	90%	90%	90%
Isolation Forest	93%	88%	89%
Minimum Covariance Determinant	94%	83%	87%
